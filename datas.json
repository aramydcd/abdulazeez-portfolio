{
    "projects": [
        {
            "title": "Personal Branding Engine & Dynamic CMS",
            "description": "A high-performance, database-driven portfolio application engineered to showcase technical projects and professional experience. This project serves as a 'Meta-Project', demonstrating the ability to build, deploy, and maintain a live web application with a dynamic backend and automated data persistence.\n\nKEY ENGINEERING CONTRIBUTIONS\n1. Dual-Environment Database Architecture\n\u2022 Hybrid Storage: Developed a flexible configuration system that utilizes SQLite for rapid local development and PostgreSQL for scalable production on Render.\n\u2022 ORM Layer: Implemented SQLAlchemy to abstract database interactions, allowing for seamless migrations and complex relational queries without writing raw SQL.\n\n2. Dynamic Content Management (CMS)\n\u2022 Relational Schema: Architected a relational database to handle one-to-many relationships between projects, technology tags, and user experiences.\n\u2022 Template Logic: Used Jinja2 to build reusable components, ensuring that adding a new project to the database automatically updates the UI, the category filters, and the project detail pages without touching the HTML.\n\n3. Frontend Excellence & Responsive UX\n\u2022 Bootstrap 5 Integration: Designed a mobile-first UI using Bootstrap 5 grid systems and utility classes for a clean, professional aesthetic.\n\u2022 Interactive Components: Leveraged JavaScript for reactive UI elements, such as dynamic project filtering and hover-state overlays.\n\nCORE FUNCTIONALITY\nFeature & Implementation\n\n1. Backend Framework: Flask with Blueprints for organized, modular routing.\n2. Persistence: SQLite (Dev) / PostgreSQL (Prod) via SQLAlchemy ORM.\n3. Styling: Custom CSS3 and Bootstrap 5 for high-fidelity responsive design.\n4. Deployment: CI/CD pipeline integrated with GitHub and Render.\n5. Environment Control: Secure handling of API keys and DB URLs via .env files.\n",
            "technologies": "Python (Flask), PostgreSQL, SQLite, SQLAlchemy, Bootstrap 5, Jinja2, Render",
            "github_url": "https://github.com/aramydcd/abdulazeez-portfolio",
            "live_demo_url": "https://abdulazeez-portfolio.onrender.com/",
            "image_file": "6b0af93e74d946ed.jpg"
        },
        {
            "title": "ElectiSync \u2013 Election Analytics & Result Management System (Bincom Test",
            "description": "ElectiSync is an interactive data management system developed to provide transparent access to regional election results. The project features a robust backend for database migration and a clean, tabbed interface for data visualization and entry. It addresses a core engineering challenge: migrating legacy MySQL datasets into a localized SQLite environment for rapid analysis and reporting.\n\nKEY ENGINEERING CONTRIBUTIONS \n1. Automated Database Migration Pipeline\nI engineered a custom migration script in _init_.py to handle cross-engine database compatibility:\n\u2022 Legacy SQL Sanitization: Developed a Regex-powered cleaning engine to strip MySQL-specific headers (e.g., ENGINE, AUTO_INCREMENT) that would otherwise crash an SQLite instance.\n\u2022 Streamlined Initialization: Implemented an automated setup process that reads raw SQL dumps and builds a high-integrity .db file in seconds using cursor.executescript.\n\n2. Multi-Level Result Analysis (Relational Logic)\n\u2022 Hierarchical Queries: Architected complex SQL joins to aggregate scores from individual Polling Units (PUs) up to the Local Government Area (LGA) level.\n\u2022 SQL Injection Prevention: Implemented secure, parameterized queries across the model layer to ensure data security during user-driven data fetching.\n\u2022 Dataframe Integration: Leveraged Pandas to bridge the gap between raw SQL results and Streamlit's UI, enabling high-speed table rendering and reactive data charts.\n\n3. Data Persistence & Integrity\n\u2022 Dynamic Result Entry: Built a secure 'Store Results' portal using Streamlit Forms, featuring automated ID generation and timestamping.\n\u2022 Audit Trail Logic: Developed a verification system to display real-time database activity, allowing administrators to track the most recent five entries for data validation.\n\nCORE FUNCTIONALITY\nFeature & Implementation\n1. Migration Engine: Automated MySQL-to-SQLite conversion via Regex cleaning.\n2. Polling Unit Result Viewer: Individual polling unit tracking with bar chart visualization.\n3. LGA Aggregator: Real-time score summation across multiple regional units.\n4. Secure Submission: Validated form-based entry for new election records.\n5. Audit Log: Real-time display of recent database transactions.",
            "technologies": "Python, Streamlit, SQLite3, Pandas, Regex (re)",
            "github_url": "https://github.com/aramydcd/bincom_python_developer_intern_interview",
            "live_demo_url": "https://bincom-python-developer-intern-assessment.streamlit.app/",
            "image_file": "d6a51d570123174e.jpg"
        },
        {
            "title": "ColorSync - Data Intelligence & Algorithmic Solver (Bincom Technical Assessment Suite \ud83d\udc0d)",
            "description": "\nColorSync was developed as a technical challenge to demonstrate proficiency in data extraction, cleaning, and mathematical modeling. The project involves a multi-stage pipeline: extracting unstructured garment color data from a simulated web source, normalizing spelling anomalies via Regular Expressions, and applying statistical algorithms to solve complex data queries. The project also features a persistence layer using SQLite and implementations of classic computer science algorithms (Recursion and Fibonacci).\n\nKEY ENGINEERING CONTRIBUTIONS\n1. Smart Data Extraction & Normalization\n\u2022 Pattern Matching: Utilized Word Boundary Anchors (\b) in Regex to ensure precise categorical extraction and avoid partial string matches. Also, Implemented Regex (re.findall) to isolate categorical data from raw HTML content, ensuring high-speed data retrieval.\n\u2022 Data Sanitization: Built a normalization layer to correct \"dirty\" data entries (e.g., remapping 'BLEW' to 'BLUE' and 'ARSH' to 'ASH'), showcasing attention to detail in data integrity.\n\u2022 Frequency Mapping: Utilized the collections.Counter library to transform raw lists into high-performance frequency dictionaries for optimized lookup.\n\n2. Statistical & Algorithmic Engine\n\u2022 Central Tendency & Variance: Implemented mathematical models to calculate the Mean (Frequency Proximity Analysis), Median (via cumulative sum logic), and Variance of color occurrences.\n\u2022 Recursive Search: Developed a manual Recursive Search Algorithm to traverse datasets, demonstrating a fundamental understanding of call-stack management and base-case logic.\n\u2022 Fibonacci Optimization: Engineered an iterative solver for the Fibonacci sequence to calculate large-scale sums (50th sequence), focusing on time complexity efficiency.\n\n3. Persistence & Computation\n\u2022 Self-Contained DB Architecture: Built an automated SQLite pipeline to persist frequency data. I chose SQLite for its zero-config nature, ensuring the application is portable and easy for reviewers to run.\n\u2022 Binary-to-Base10 Logic: Implemented a random-generation bitwise conversion tool to demonstrate proficiency with different number systems.\n\nCORE FUNCTIONALITY\nChallenge & Technical Implementation\n1. Data Cleaning: Regex-based extraction + spelling normalization logic.\n2. Statistical Analysis: Mean, Mode, Median (Cumulative Sum), and Variance.\n3. Persistence: SQLite integration for automated frequency storage.\n4. Algorithms: Recursive search and iterative Fibonacci summation.\n5. Data Logic: Binary-to-Integer conversion and probability modeling.\n",
            "technologies": "Python, Regex (re), SQLite3, Statistics, Collections (Counter), Git",
            "github_url": "https://github.com/aramydcd/bincom-preliminary-python-interview",
            "live_demo_url": "https://app.com",
            "image_file": "337c66e8eb809b7d.jpg"
        },
        {
            "title": "EduShield \u2013 Predictive Academic Analytics & Resource Hub",
            "description": "\nEduShield is a data-centric academic monitoring platform designed to provide proactive intervention strategies for student success. Beyond traditional management, the system utilizes Machine Learning to predict student outcomes based on historical performance data. It serves as a real-time analytics hub where students and lecturers can visualize academic trends, manage resources, and engage in collaborative group messaging.\n\nKEY ENGINEERING CONTRIBUTIONS \n1. Predictive GPA Modeling (Machine Learning Integration):\nI engineered a predictive backend that moves beyond static data reporting:\n\u2022 Outcome Forecasting: Developed a regression-based model using to predict future GPA results based on current grade trends and attendance metrics.\n\u2022 Proactive Alerts: Designed the logic to flag \"at-risk\" students, allowing for early academic intervention.\n\n2. Real-Time Data Processing & Analytics Pipeline\nBuilt a robust data pipeline to handle live academic metrics:\n\u2022 Dynamic Analytics: Leveraged Pandas and NumPy to process raw SQLite data into real-time performance visualizations, including grade distribution and attendance heatmaps.\n\u2022 On-the-Fly Export Logic: Implemented a specialized data extraction module allowing users to generate and download customized CSV reports for enrollment, attendance, and grading tables.\n\n3. Content Management & Collaborative Infrastructure\n\u2022 Binary Resource Handling: Developed a secure file-serving system for lecturers to upload and distribute academic materials (PDFs, DOCs) with automated file-naming conventions and metadata mapping.\n\u2022 State-Managed Messaging: Architected a group messaging system within the Streamlit framework, utilizing session-state management to ensure real-time communication between academic cohorts.\n\nCORE FUNCTIONALITY \nFeature & Implementation\n1. GPA Prediction: ML-driven forecasting engine using Scikit-learn.\n2. Performance Analytics: Real-time data visualization of academic KPIs.\n3. Resource Repository: Centralized PDF/DOC management for course materials.\n4. Data Portability: Bi-directional CSV export for students and lecturers.\n5. Real-time Comms: Integrated group messaging using session-state logic.\n\nTHE IMPACT\nEduShield highlights my versatility as a developer. It demonstrates that I can bridge the gap between Software Engineering and Data Science, creating tools that not only store data but interpret it to provide actionable insights.\n",
            "technologies": "Python, Streamlit, SQL/SQLite, Pandas, NumPy, Matplotlib, Bcrypt, Statistics",
            "github_url": "https://github.com/aramydcd/Student-Performance-Monitoring-System",
            "live_demo_url": "https://edushield.streamlit.app/",
            "image_file": "d64a801b3ebf8539.jpg"
        },
        {
            "title": "AcadSync \u2013 Integrated Academic Management & Workflow System",
            "description": "\nAcadSync is a comprehensive academic ERP solution designed to centralize university operations and automate the administrative lifecycle. The platform replaces fragmented manual processes with a unified digital ecosystem, enabling real-time collaboration between students, lecturer, and administrators. The system is engineered to handle complex academic workflows, from course registration and grade management to faculty workload allocation.\n\nKEY ENGINEERING CONTRIBUTIONS:\n1. Multi-Tiered Role-Based Access Control (RBAC):\nI architected a sophisticated permission system that dynamically serves three distinct user experiences from a single backend:\n\u2022 Student Portal: Integrated course enrollment logic, real-time result tracking, and GPA Trends visualization.\n\u2022 Lecturer Suite: Developed grade-entry interfaces and course progress analytics.\n\u2022 Admin Command Center: Built a \"Superuser\" dashboard for departmental management, course creation, and user auditing.\n\n2. Relational Schema Optimization:\nDesigned a high-integrity database architecture to manage the \"Many-to-Many\" and \"One-to-Many\" relationships inherent in academic data:\n\u2022 Conflict Resolution: Engineered logic to prevent course clashing during student enrollment and course allocation.\n\u2022 Data Integrity: Implemented foreign key constraints and atomic transactions to ensure that academic records remain consistent and tamper-proof across high-traffic registration periods.\n\n3. Responsive Dashboards & API Architecture:\n\u2022 Real-time Analytics: Integrated Chart.js with Django's backend to provide students and lecturers with visual representations of GPA trends and course progress analytics.\n\u2022 Secure API Design: Developed RESTful endpoints for Client-server synchronization, ensuring that sensitive student records are only accessible through authorized requests.\n\n4. Conditional Logic & Eligibility Engine:\nI engineered a strict \"Gatekeeper\" logic to automate academic compliance and exam regulation:\n\u2022 Attendance Tracking: Built a daily attendance management module where lecturers can log presence, which the backend then aggregates into a real-time percentage for each student.\n\u2022 Automated Exam Eligibility: Developed a conditional validation engine that cross-references attendance records with departmental requirements (e.g., 75% threshold).\n\u2022 Dynamic Access Control: Students who fall below the attendance threshold are automatically flagged by the system, and their exam registration or admit cards are restricted via backend permission checks.\n\nCORE FUNCTIONALITY\nFeature & Implementation\n1. Course Allocation: Automated mapping of lecturers to modules based on departmental data.\n2. Result Management: Secure grade computation and automated GPA calculation engine.\n3. Workflow Automation: Real-time tracking of academic status and enrollment eligibility.\n4. Centralized Database: Optimized SQL schema for seamless data flow between user roles.\n5. Attendance & Eligibility: Rule-based engine that locks exam access based on real-time attendance thresholds.\n\nTHE IMPACT\nAcadSync proves my ability to design complex, multi-user systems that require deep logical thinking. It showcases my skill in translating intricate institutional rules (like grading scales and enrollment prerequisites) into clean, maintainable Python code and efficient SQL queries.\n",
            "technologies": "Python, Django, SQL/SQLite, HTML5, Bootstrap 5, CSS3, JavaScript, Django-Auth, Chart.js, Git",
            "github_url": "https://github.com/aramydcd/student-information-system",
            "live_demo_url": "https://myapp.com",
            "image_file": "161f10a2b1bdb374.jpg"
        },
        {
            "title": "\ud83d\udc8a DrugVerify \u2013 Global Pharmaceutical Traceability & Authentication System",
            "description": "\n\ud83d\udc8a DrugVerify is a high-integrity web ecosystem engineered to eliminate pharmaceutical counterfeiting and supply chain fraud. By bridging the gap between manufacturers and the end consumer, the platform provides a \"Single Source of Truth\" for medication authenticity. The system is architected to handle complex data relationships while offering a seamless, mobile-first experience for field verification.\n\nKEY ENGINEERING CONTRIBUTIONS:\n1. Full-Stack CRUD Architecture (Manufacturer Suite):\nDesigned and implemented a secure, role-based management dashboard for verified pharmaceutical manufacturers:\n    \u2022 Inventory Control: Enables manufacturers to perform full CRUD (Create, Read, Update, Delete) operations on drug profiles, batch records, and unique identifiers.\n    \u2022 Data Governance: Implemented strict ownership logic to ensure manufacturers can only modify their own authorized datasets, utilizing Django\u2019s robust ORM and middleware.\n\n2. Hybrid Computer Vision Verification Pipeline:\nTo ensure 100% availability across all devices, I engineered a dual-channel scanning solution:\n    \u2022 Frontend (Real-Time): Integrated a JavaScript-based camera scanner Implementing the 'html5-qrcode' library with custom event-handling for automated form submission and for instantaneous, client-side authentication.\n    \u2022 Backend (High-Performance): Developed a fail-safe Python pipeline. When users upload images, the backend reads the `InMemoryUploadedFile` as a raw byte stream into a NumPy buffer, processing it via OpenCV (`cv2`) for server-side decoding. This eliminates disk I/O bottlenecks and optimizes RAM usage.\n\n3. Security & Integrity Protocols:\n    \u2022 Audit Logging: Engineered a persistent audit trail in an immutable log, recording user metadata, timestamps, and validation results to help regulatory bodies identify hotspots of counterfeit activity.\n    \u2022 Session Management: Implemented multi-level authentication and secure session handling to protect sensitive manufacturer data and administrative controls.\n    \u2022 Scalable Schema: Architected a relational database schema that handles complex \"One-to-Many\" relationships between Manufacturers and their respective Drug Batches.\n\nCORE FUNCTIONALITY\nFeatures & Implementation:\n1. Manufacturer Dashboard: Secure CRUD operations for drug and batch management.\n2. Real-time Authentication: Live camera-based QR scanning with auto-submit logic.\n3. Server-side Processing: OpenCV-powered image decoding for uploaded pack photos.\n4. Traceability: Unique batch-to-manufacturer linking and verification history.\n\nTHE IMPACT\nThe result is a production-ready platform that solves a real-world problem. It demonstrates my ability to handle binary data processing, asynchronous frontend events, and secure backend architecture within a single, unified product.\n",
            "technologies": "Python, Django, OpenCV, NumPy, SQL/SQLite, HTML5, Bootstrap 5, CSS3, JavaScript (html5-qrcode), Git, UV",
            "github_url": "https://github.com/aramydcd/Drug-Verification-System",
            "live_demo_url": "https://myapp.com",
            "image_file": "f937173b4c878702.jpg"
        }
    ],
    "experiences": [
        {
            "job_title": "Software Developer Intern (Backend Focus)",
            "company": "4Real Global IT Solution | Innovative ICT Firm",
            "duration": "Dec 2024 - Mar 2025",
            "technologies": "VB.NET, Microsoft Visual Studio 2010",
            "description": "\u2022 Financial Systems Optimization: Engineered high-precision server-side logic using VB.NET, reducing calculation latency by 15% for enterprise-level financial tools.\n\u2022 Scalable Architecture: Leveraged Object-Oriented Programming (OOP) to refactor legacy code into modular, reusable components, decreasing technical debt and future maintenance time.\n\u2022 SDLC Lifecycle Management: Actively contributed to the full Software Development Life Cycle, implementing rigorous debugging and unit testing protocols that improved system reliability by 20%.\n\u2022 Data Integrity & Security: Designed and maintained robust data structures for desktop service environments, ensuring 100% stability across multi-user financial transactions.\n\u2022 Cross-Functional Collaboration: Translated complex business requirements into technical specifications, bridging the gap between stakeholder needs and backend implementation.\n"
        }
    ],
    "skills": [
        {
            "category": "Backend",
            "description": "Architecting secure APIs and logic using Python and Flask.",
            "tools": "Python, Flask, Django, API's Integration, DRF, SQL, PostgreSQL, SQLite",
            "icon_class": "bi-code-slash"
        },
        {
            "category": "Frontend",
            "description": "Building responsive, user-first interfaces with modern CSS.",
            "tools": "HTML5, CSS3, Bootstrap 5, JavaScript, Streamlit",
            "icon_class": "bi-layout-sidebar"
        },
        {
            "category": "DevOps/Tools",
            "description": "Version control, deployment, and environment management.",
            "tools": "Git/GitHub, CI/CD, UV, Docker (Containerization), Database Schema Design, SQL Normalization, ETL (Migration Pipelines), Vercel, Streamlit Cloud, Render",
            "icon_class": "bi-terminal"
        }
    ],
    "target_roles": [
        {
            "title": "Backend Developer",
            "icon_class": "bi-server",
            "description": "Focused on building scalable server-side logic...",
            "cv_filename":"backend_developer_Abdulazeez_Abdulakeem_Backend_Developer_CV.pdf"
        },
        {
            "title": "Database Administrator",
            "icon_class": "bi-database-check",
            "description": "Interests in SQL optimization..."
        },
        {
            "title": "Full-Stack Engineer",
            "icon_class": "bi-window-stack",
            "description": "Bridging the gap between backends and frontends..."
        }
    ]
}